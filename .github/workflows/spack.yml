name: Spack

on:
  push:
    branches:
      - main
  pull_request:
  workflow_dispatch:

permissions:
  packages: write
  contents: write

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  # Note that each spack version needs its own registry
  SPACK_VERSION: v1.0.2
  REGISTRY: ghcr.io/awslabs/palace
  GITHUB_USER: ${{ github.actor }}
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

jobs:
  # Filter to allow skipping the test if no relevant changes occurred.
  filter:
    runs-on: ubuntu-latest
    outputs:
      test: ${{ steps.filter.outputs.test }}
    steps:
      - uses: actions/checkout@v4
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            test:
              - 'palace/**'
              - 'cmake/**'
              - 'extern/**'
              - 'examples/**'
              - 'test/examples/**'
              - 'test/unit/**'
              - '.github/workflows/spack.yml'

  build-and-test-spack:
    needs: filter
    strategy:
      fail-fast: false # don't have one build stop others from finishing
      matrix:
        include:
          # Instead of testing every combination of the various matrix options,
          # we do all-pairs testing. This means that we ensure we cover every
          # single pair of parameter values at least once.

          # The following YAML block is generated with the
          # generate_test_matrix.py script, which also enforces various
          # constraints (given that not all the combination of parameters make
          # sense, e.g., armpl with x86)

        - arch: x86
          compiler: gcc
          mpi: openmpi
          math-libs: openblas
          shared: +shared
          int: +int64
          openmp: ~openmp
          eigensolver: +arpack
          solver: +mumps
          cuda: ~cuda

        # Fails configuring PETSc
        # - arch: x86
        #   compiler: llvm
        #   mpi: mpich
        #   math-libs: amdblis
        #   shared: ~shared
        #   int: ~int64
        #   openmp: +openmp
        #   eigensolver: +slepc
        #   solver: +superlu-dist
        #   cuda: ~cuda

        # This requires the env_vars file to work, otherwise fails building MFEM
        # with `FC` not found
        # The floquet/periodic regression test fails
        # - arch: arm
        #   compiler: llvm
        #   mpi: openmpi
        #   math-libs: openblas
        #   shared: ~shared
        #   int: ~int64
        #   openmp: ~openmp
        #   eigensolver: +slepc
        #   solver: +strumpack
        #   cuda: ~cuda

        - arch: arm
          compiler: gcc
          mpi: mpich
          math-libs: armpl-gcc
          shared: +shared
          int: +int64
          openmp: +openmp
          eigensolver: +arpack
          solver: +strumpack
          cuda: ~cuda

        - arch: x86
          compiler: intel-oneapi-compilers
          mpi: intel-oneapi-mpi
          math-libs: intel-oneapi-mkl
          shared: +shared
          int: +int64
          openmp: ~openmp
          eigensolver: +slepc
          solver: +superlu-dist
          cuda: ~cuda

        - arch: x86
          compiler: intel-oneapi-compilers
          mpi: intel-oneapi-mpi
          math-libs: intel-oneapi-mkl
          shared: ~shared
          int: ~int64
          openmp: +openmp
          eigensolver: +arpack
          solver: +mumps
          cuda: ~cuda

        - arch: x86
          compiler: llvm
          mpi: mpich
          math-libs: amdblis
          shared: +shared
          int: +int64
          openmp: ~openmp
          eigensolver: +arpack
          solver: +mumps
          cuda: ~cuda

        # Fails building superlu with pointer conversion error int vs int_t
        # - arch: x86
        #   compiler: gcc
        #   mpi: openmpi
        #   math-libs: amdblis
        #   shared: ~shared
        #   int: +int64
        #   openmp: +openmp
        #   eigensolver: +arpack
        #   solver: +superlu-dist
        #   cuda: ~cuda

        # Fails with undefined reference to symbol 'sqrt@@GLIBC_2.17' in libmetis
        # - arch: arm
        #   compiler: gcc
        #   mpi: openmpi
        #   math-libs: armpl-gcc
        #   shared: ~shared
        #   int: ~int64
        #   openmp: ~openmp
        #   eigensolver: +slepc
        #   solver: +mumps
        #   cuda: ~cuda

        - arch: arm
          compiler: llvm
          mpi: mpich
          math-libs: openblas
          shared: +shared
          int: ~int64
          openmp: +openmp
          eigensolver: +arpack
          solver: +superlu-dist
          cuda: ~cuda

        - arch: x86
          compiler: gcc
          mpi: openmpi
          math-libs: openblas
          shared: +shared
          int: ~int64
          openmp: +openmp
          eigensolver: +slepc+arpack
          solver: +superlu-dist+mumps+sundials+strumpack
          cuda: ~cuda

    runs-on: ${{ matrix.arch == 'x86' && 'palace_ubuntu-latest_16-core' || 'ubuntu-24.04-arm' }}
    steps:
      - uses: actions/checkout@v4
        if: needs.filter.outputs.test == 'true'

      - name: Setup Spack
        if: needs.filter.outputs.test == 'true'
        uses: spack/setup-spack@v2
        with:
          ref: ${{ env.SPACK_VERSION }}
          buildcache: true
          color: true

      - name: Setup Environment
        if: needs.filter.outputs.test == 'true'
        run: |
          # Change defaults for solver and eigensolvers (by not including them in the build).
          if [[ "${{ matrix.solver }}" == "+strumpack" ]]; then
            SOLVER_VARIANTS="+strumpack~superlu-dist~mumps"
          elif [[ "${{ matrix.solver }}" == "+mumps" ]]; then
            SOLVER_VARIANTS="+mumps~superlu-dist~strumpack"
          elif [[ "${{ matrix.solver }}" == "+superlu-dist" ]]; then
            SOLVER_VARIANTS="+superlu-dist~strumpack~mumps"
          else
            SOLVER_VARIANTS="${{ matrix.solver }}"
          fi

          if [[ "${{ matrix.eigensolver }}" == "+arpack" ]]; then
            EIGEN_VARIANTS="+arpack~slepc"
          elif [[ "${{ matrix.eigensolver }}" == "+slepc" ]]; then
            EIGEN_VARIANTS="+slepc~arpack"
          else
            EIGEN_VARIANTS="${{ matrix.eigensolver }}"
          fi
          
          PALACE_SPEC="local.palace@develop${SOLVER_VARIANTS}${EIGEN_VARIANTS}${{ matrix.shared }}${{ matrix.int }}${{ matrix.openmp }}${{ matrix.cuda }}+tests"

          # LLVM does not provide flang by default, so we use gcc
          if [[ "${{ matrix.compiler }}" == "llvm" ]]; then
            FORTRAN_COMPILER="gcc"
          else
            FORTRAN_COMPILER="${{ matrix.compiler }}"
          fi

          # Spack.yaml with most / all settings configured
          cat << EOF > spack.yaml
          spack:
            specs:
              - ${PALACE_SPEC}
            view: false
            config:
              install_tree:
                root: /opt/spack
                padded_length: false
            concretizer:
              reuse: false
              unify: true
              # generic cpu target so that we can increase cache hits
              targets:
                granularity: generic
            packages:
              petsc:
                require: ~hdf5
              mpi:
                require: ${{ matrix.mpi }}
              blas:
                require: ${{ matrix.math-libs }}
              c:
                require: [${{ matrix.compiler }}]
              cxx:
                require: [${{ matrix.compiler }}]
              fortran:
                require: [${FORTRAN_COMPILER}]
              # Metis is verify fragile
              # -fPIC seems to be required because metis always compiles some shared objects
              # -lc seems to be required for intel compilers
              metis:
                require: cflags=-fPIC cppflags=-fPIC ldflags=-lc
              parmetis:
                require: ldflags=-lc
            repos:
            - spack_repo/local
            mirrors:
              spack:
                binary: true
                url: https://binaries.spack.io/${SPACK_VERSION}
              local-buildcache:
                binary: true
                url: oci://${{ env.REGISTRY }}-${SPACK_VERSION}
                signed: false
                access_pair:
                  id_variable: GITHUB_USER
                  secret_variable: GITHUB_TOKEN
          EOF

          # MFEM with LLVM+gfortran fails with FC not found without this,
          # and compiler-wrapper does not recognize ifx otherwise.
          if [[ "${{ matrix.compiler }}" != gcc ]]; then
            FC_PATH=$(spack compiler info ${FORTRAN_COMPILER} | grep 'fortran:' | awk '{print $2}')
            cat << ENVEOF > env_vars.yaml
          env_vars:
            set:
              FC: ${FC_PATH}
              SPACK_FC: ${FC_PATH}
              SPACK_F77: ${FC_PATH}
          ENVEOF
          fi

      # Workaround for https://github.com/spack/spack/issues/51505
      # 
      # Currently not needed because we have a full copy of mfem and libceed
      # because of https://github.com/spack/spack-packages/pull/2361

      # - name: Overwrite builtin Palace package
      #   if: needs.filter.outputs.test == 'true'
      #   run: |
      #     ln -s "$(spack location --repo builtin)/packages/libceed" spack_repo/local/packages/
      #     ln -s "$(spack location --repo builtin)/packages/mfem" spack_repo/local/packages/

      - name: Remove Android NDK # it confuses the Intel compiler
        if: needs.filter.outputs.test == 'true' && matrix.compiler == 'intel-oneapi-compilers'
        run: sudo rm -rf /usr/local/lib/android/

      - name: Install Intel repos and configure Intel MPI
        if: needs.filter.outputs.test == 'true' && matrix.mpi == 'intel-oneapi-mpi'
        uses: mpi4py/setup-mpi@v1
        with:
          mpi: intelmpi

      - name: Configure Intel oneAPI compiler
        if: needs.filter.outputs.test == 'true' && matrix.compiler == 'intel-oneapi-compilers'
        run: |
          sudo apt-get install -y intel-oneapi-compiler-dpcpp-cpp \
                                  intel-oneapi-compiler-fortran \
                                  intel-oneapi-mpi \
                                  intel-oneapi-mpi-devel

      - name: Install OpenMP for LLVM
        if: needs.filter.outputs.test == 'true' && matrix.compiler == 'llvm'
        run: sudo apt-get install -y libomp-dev

      - name: Install OpenMP for GCC
        if: needs.filter.outputs.test == 'true' && matrix.compiler == 'llvm'
        run: sudo apt-get install -y libgomp1

      - name: Configure External Packages and compilers
        if: needs.filter.outputs.test == 'true'
        run: |
          # Spack needs a little help finding the intel compiler
          if [[ "${{ matrix.compiler }}" == "intel-oneapi-compilers" ]]; then
            . /opt/intel/oneapi/setvars.sh
          fi

          # These cause build issues if built as externals
          #   - python : often distributed python isn't feature complete / not all dependencies get detected
          #   - OpenSSL / OpenSSH : since they are in /usr, spack struggles. It's common to rebuild these
          #   - ncurses / bzip2 / xz / curl : caused build issues. We pull these from GHCR cache after first build
          spack -e . external find --all \
            --exclude openssl \
            --exclude openssh \
            --exclude python \
            --exclude ncurses \
            --exclude bzip2 \
            --exclude xz \
            --exclude curl

          spack -e . compiler find && spack -e . compiler list

      - name: Configure Binary Mirror Keys
        if: needs.filter.outputs.test == 'true'
        run: |
          # If we cached these, that would be faster and safer
          spack -e . buildcache keys --install --trust

      - name: Bootstrap
        if: needs.filter.outputs.test == 'true'
        run: spack -e . bootstrap now

      - name: Concretize
        if: needs.filter.outputs.test == 'true'
        # In theory we can re-use a concretization and pin a spack to speed this up.
        # Unfortunately it then becomes difficult to know when to re-concretize.
        run: |
          # Using `spack develop` in order to have an in-source build
          spack -e . develop --path=$(pwd) palace@git."${{ github.head_ref || github.ref_name }}"=develop
          spack -e . concretize -f

      - name: Build Dependencies
        if: needs.filter.outputs.test == 'true'
        run: |
          spack -e . install --only-concrete --no-check-signature --fail-fast --show-log-on-error --only dependencies

      - name: Build Palace
        if: needs.filter.outputs.test == 'true'
        # Build palace from source using this current directory
        # We use `--no-cache` in order to force a rebuild
        run: spack -e . install --only-concrete --keep-stage --show-log-on-error --only package --no-cache

      - name: Run Unit Tests
        if: needs.filter.outputs.test == 'true'
        env:
          OMP_NUM_THREADS: 2
        run: |
          eval $(spack -e . load --sh palace)
          palace-unit-tests --skip-benchmarks
          mpirun -np 2 $(which palace-unit-tests) --skip-benchmarks

      - uses: julia-actions/setup-julia@v2
        if: needs.filter.outputs.test == 'true'
        with:
          version: '1'

      - uses: julia-actions/cache@v2
        if: needs.filter.outputs.test == 'true'

      - name: Run Integration Tests
        if: needs.filter.outputs.test == 'true'
        run: |
          # palace_ubuntu-latest_16-core seems to have hyperthreading, so it is
          # really 8 cores, not 16
          if [[ "${{ matrix.arch }}" == "x86" ]]; then
            export NUM_PROC_TEST=8
          else
            export NUM_PROC_TEST=$(nproc 2> /dev/null || sysctl -n hw.ncpu)
          fi

          if [[ "${{ matrix.with-openmp }}" == '+openmp' ]]; then
            NUM_PROC_TEST=$(( NUM_PROC_TEST / 2 ))
            export OMP_NUM_THREADS=2
          else
            export OMP_NUM_THREADS=1
          fi

          eval $(spack -e . load --sh palace)
          # Run tests
          julia --project=test/examples -e 'using Pkg; Pkg.instantiate()'
          julia --project=test/examples --color=yes test/examples/runtests.jl

      # Push built binaries, even if the build fails
      # NOTE: This might fail as external fork PRs can't push to GHCR.
      - name: Push to GHCR cache
        if: |
          needs.filter.outputs.test == 'true' &&
          !cancelled() &&
          (github.event_name != 'pull_request' || github.event.pull_request.head.repo.full_name == github.repository)
        run: spack -e . buildcache push --force --with-build-dependencies --unsigned --update-index local-buildcache
