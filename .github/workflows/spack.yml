name: Spack

on:
  push:
    branches:
      - main
  pull_request:
  workflow_dispatch:

permissions:
  packages: write
  contents: write

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  # Note that each spack version needs its own registry
  SPACK_VERSION: develop
  REGISTRY: ghcr.io/awslabs/palace
  GITHUB_USER: ${{ github.actor }}
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

jobs:
  # Filter to allow skipping the test if no relevant changes occurred.
  filter:
    runs-on: ubuntu-latest
    outputs:
      test: ${{ steps.filter.outputs.test }}
    steps:
      - uses: actions/checkout@v6
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            test:
              - 'palace/**'
              - 'cmake/**'
              - 'extern/**'
              - 'examples/**'
              - 'test/examples/**'
              - 'test/unit/**'
              - '.github/workflows/spack.yml'

  build-and-test-spack:
    needs: filter
    strategy:
      fail-fast: false # don't have one build stop others from finishing
      matrix:
        include:
          - arch: x86
            compiler: gcc
            mpi: openmpi
            math-libs: openblas
            shared: +shared
            int: +int64
            openmp: ~openmp
            eigensolver: +arpack
            solver: +mumps
            cuda: ~cuda
            runner: palace_ubuntu-latest_16-core

          # SUNDIALS 7.5.0 fails with
          # Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAM
          # ES)
          #
          # (Probably fixable by forcing LLVM@19)
          # - arch: x86
          #   compiler: llvm
          #   mpi: mpich
          #   math-libs: amdblis
          #   shared: ~shared
          #   int: ~int64
          #   openmp: +openmp
          #   eigensolver: +slepc
          #   solver: +superlu-dist
          #   cuda: ~cuda
          #   runner: palace_ubuntu-latest_16-core

          # OpenBlas 0.3.30 fails with
          # FATAL ERROR - COMPUTED RESULT IS LESS THAN HALF ACCURATE
          # 
          # - arch: arm
          #   compiler: llvm
          #   mpi: openmpi
          #   math-libs: openblas
          #   shared: ~shared
          #   int: ~int64
          #   openmp: ~openmp
          #   eigensolver: +slepc
          #   solver: +strumpack
          #   cuda: ~cuda
          #   runner: [self-hosted, arm64]

          - arch: arm
            compiler: gcc
            mpi: mpich
            math-libs: armpl-gcc
            shared: +shared
            int: +int64
            openmp: +openmp
            eigensolver: +arpack
            solver: +strumpack
            cuda: ~cuda
            runner: [self-hosted, arm64]

          - arch: x86
            compiler: intel-oneapi-compilers
            mpi: intel-oneapi-mpi
            math-libs: intel-oneapi-mkl
            shared: +shared
            int: +int64
            openmp: ~openmp
            eigensolver: +slepc
            solver: +superlu-dist
            cuda: ~cuda
            runner: palace_ubuntu-latest_16-core

          # Mumps fails with multiple definition of `main'
          # 
          # - arch: x86
          #   compiler: intel-oneapi-compilers
          #   mpi: intel-oneapi-mpi
          #   math-libs: intel-oneapi-mkl
          #   shared: ~shared
          #   int: ~int64
          #   openmp: +openmp
          #   eigensolver: +arpack
          #   solver: +mumps
          #   cuda: ~cuda
          #   runner: palace_ubuntu-latest_16-core

          - arch: x86
            compiler: llvm
            mpi: mpich
            math-libs: amdblis
            shared: +shared
            int: +int64
            openmp: ~openmp
            eigensolver: +arpack
            solver: +mumps
            cuda: +cuda
            runner: [self-hosted, gpu]

          # Performing Test STRUMPACK_VERSION_OK - Failed
          # 
          # - arch: x86
          #   compiler: gcc
          #   mpi: openmpi
          #   math-libs: amdblis
          #   shared: ~shared
          #   int: ~int64
          #   openmp: +openmp
          #   eigensolver: +slepc
          #   solver: +strumpack
          #   cuda: ~cuda
          #   runner: palace_ubuntu-latest_16-core

          - arch: arm
            compiler: gcc
            mpi: openmpi
            math-libs: armpl-gcc
            shared: ~shared
            int: ~int64
            openmp: ~openmp
            eigensolver: +slepc
            solver: +superlu-dist
            cuda: ~cuda
            runner: [self-hosted, arm64]

          # cylinder/floquet (periodic) fails
          #
          # - arch: arm
          #   compiler: gcc
          #   mpi: openmpi
          #   math-libs: openblas
          #   shared: ~shared
          #   int: +int64
          #   openmp: +openmp
          #   eigensolver: +slepc
          #   solver: +mumps
          #   cuda: ~cuda
          #   runner: [self-hosted, arm64]

          - arch: arm
            compiler: gcc
            mpi: openmpi
            math-libs: openblas
            shared: ~shared
            int: ~int64
            openmp: +openmp
            eigensolver: +slepc
            solver: +superlu-dist
            cuda: ~cuda
            runner: [self-hosted, arm64]

          - arch: arm
            compiler: gcc
            mpi: openmpi
            math-libs: armpl-gcc
            shared: ~shared
            int: ~int64
            openmp: +openmp
            eigensolver: +slepc
            solver: +mumps
            cuda: ~cuda
            runner: [self-hosted, arm64]

          - arch: x86
            compiler: gcc
            mpi: openmpi
            math-libs: openblas
            shared: +shared
            int: ~int64
            openmp: ~openmp
            eigensolver: +slepc+arpack
            solver: +superlu-dist+mumps+sundials+strumpack
            cuda: ~cuda
            runner: palace_ubuntu-latest_16-core

          - arch: x86
            compiler: gcc
            mpi: openmpi
            math-libs: openblas
            shared: +shared
            int: ~int64
            openmp: ~openmp
            eigensolver: +slepc+arpack
            solver: +superlu-dist+mumps+sundials+strumpack
            cuda: +cuda
            runner: [self-hosted, gpu]

          - arch: x86
            compiler: gcc
            mpi: openmpi
            math-libs: openblas
            shared: +shared
            int: +int64
            openmp: ~openmp
            eigensolver: +slepc
            solver: +superlu-dist
            cuda: +cuda
            runner: [self-hosted, gpu]

          # Performing Test STRUMPACK_VERSION_OK - Failed
          # 
          # Requires
          # https://github.com/spack/spack-packages/pull/2580
          # - arch: x86
          #   compiler: llvm
          #   mpi: openmpi
          #   math-libs: amdblis
          #   shared: ~shared
          #   int: ~int64
          #   openmp: ~openmp
          #   eigensolver: +arpack
          #   solver: +strumpack
          #   cuda: +cuda
          #   runner: [self-hosted, gpu]

    runs-on: ${{ matrix.runner }}
    steps:
      - name: Print Test Matrix
        if: needs.filter.outputs.test == 'true'
        run: |
          echo "Matrix values:"
          echo "  arch: ${{ matrix.arch }}"
          echo "  compiler: ${{ matrix.compiler }}"
          echo "  mpi: ${{ matrix.mpi }}"
          echo "  math-libs: ${{ matrix.math-libs }}"
          echo "  shared: ${{ matrix.shared }}"
          echo "  int: ${{ matrix.int }}"
          echo "  openmp: ${{ matrix.openmp }}"
          echo "  eigensolver: ${{ matrix.eigensolver }}"
          echo "  solver: ${{ matrix.solver }}"
          echo "  cuda: ${{ matrix.cuda }}"

      - uses: actions/checkout@v6
        if: needs.filter.outputs.test == 'true'

      - name: Setup environment
        if: needs.filter.outputs.test == 'true'
        uses: ./.github/actions/setup-palace-ci
        with:
          julia-version: 'release'
          # Spack develop because we want to have access to the most recent version of spack-packages
          spack-version: 'develop'
          # LLVM 19 because it is the latest supported by CUDA 12.9
          llvm-version: ${{ matrix.compiler == 'llvm' && '19' || 'dont-install' }}
          setup-intel: ${{ matrix.compiler == 'intel-oneapi-compilers' && 'true' || 'false' }}

      - name: Remove Android NDK # it confuses the Intel compiler
        if: needs.filter.outputs.test == 'true' && matrix.compiler == 'intel-oneapi-compilers'
        run: sudo rm -rf /usr/local/lib/android/

      - name: Prepare spack.yaml
        if: needs.filter.outputs.test == 'true'
        run: |
          # Change defaults for solver and eigensolvers (by not including them in the build).
          if [[ "${{ matrix.solver }}" == "+strumpack" ]]; then
            SOLVER_VARIANTS="+strumpack~superlu-dist~mumps"
          elif [[ "${{ matrix.solver }}" == "+mumps" ]]; then
            SOLVER_VARIANTS="+mumps~superlu-dist~strumpack"
          elif [[ "${{ matrix.solver }}" == "+superlu-dist" ]]; then
            SOLVER_VARIANTS="+superlu-dist~strumpack~mumps"
          else
            SOLVER_VARIANTS="${{ matrix.solver }}"
          fi

          if [[ "${{ matrix.eigensolver }}" == "+arpack" ]]; then
            EIGEN_VARIANTS="+arpack~slepc"
          elif [[ "${{ matrix.eigensolver }}" == "+slepc" ]]; then
            EIGEN_VARIANTS="+slepc~arpack"
          else
            EIGEN_VARIANTS="${{ matrix.eigensolver }}"
          fi

          if [[ "${{ matrix.cuda }}" == "+cuda" ]]; then
            # nvidia-smi gives us 8.6, so we need to remove the dot
            CUDA_ARCH=$(nvidia-smi --query-gpu=compute_cap --format=csv,noheader | head -1 | tr -d '.')
            GPU_VARIANTS="+cuda cuda_arch=${CUDA_ARCH}"
          else
            GPU_VARIANTS="${{ matrix.cuda }}"
          fi

          # As of version 1.2.0-alpha, the spack concretizer hangs when using flang.
          # TODO: Try again in the future to see if we can use it.
          if [[ "${{ matrix.compiler }}" == "llvm" ]]; then
            FORTRAN_COMPILER="gcc"
          else
            FORTRAN_COMPILER="${{ matrix.compiler }}"
          fi

          PALACE_SPEC="local.palace@develop+tests${SOLVER_VARIANTS}${EIGEN_VARIANTS}${{ matrix.shared }}${{ matrix.int }}${{ matrix.openmp }}${GPU_VARIANTS}"

          # Spack.yaml with most / all settings configured
          cat << EOF > spack.yaml
          spack:
            specs:
              - ${PALACE_SPEC}
            view: false
            config:
              install_tree:
                root: ${HOME}/opt/spack
                padded_length: 256
            concretizer:
              reuse: false
              unify: true
              # generic cpu target so that we can increase cache hits
              targets:
                granularity: generic
            packages:
              petsc:
                require: ~hdf5
              mpi:
                require: ${{ matrix.mpi }}
              blas:
                require: ${{ matrix.math-libs }}
              c:
                require: [${{ matrix.compiler }}]
              cxx:
                require: [${{ matrix.compiler }}]
              fortran:
                require: [${FORTRAN_COMPILER}]
              # Metis is verify fragile
              # -fPIC seems to be required because metis always compiles some shared objects
              # -lc seems to be required for intel compilers
              metis:
                require: cflags=-fPIC cppflags=-fPIC ldflags=-lc
              parmetis:
                require: ldflags=-lc
          EOF

          # Add CUDA configuration if using CUDA
          if [[ "${{ matrix.cuda }}" == "+cuda" ]]; then
            cat << CUDAEOF >> spack.yaml
              cuda:
                buildable: false
                externals:
                - spec: cuda@12.9
                  prefix: /usr/local/cuda-12.9/
          CUDAEOF
          fi

          # Continue with the rest of the spack.yaml
          cat << EOF >> spack.yaml
            repos:
            - spack_repo/local
            mirrors:
              spack:
                binary: true
                url: https://binaries.spack.io/${SPACK_VERSION}
              local-buildcache:
                binary: true
                url: oci://${{ env.REGISTRY }}-${SPACK_VERSION}-testing
                signed: false
                access_pair:
                  id_variable: GITHUB_USER
                  secret_variable: GITHUB_TOKEN
          EOF

      - name: Patch Spack Packages for known problems
        if: needs.filter.outputs.test == 'true'
        run: |
          PALACE_DIR=$PWD
          cd $(spack location --repo builtin)
          for patch in "$PALACE_DIR"/spack_repo/patches/*.patch; do
            if [ -f "$patch" ]; then
              echo "Applying patch: $patch"
              git apply "$patch"
            fi
          done

      - name: Configure Compilers
        if: needs.filter.outputs.test == 'true'
        run: |
          # Spack needs a little help finding the intel compiler.
          if [[ "${{ matrix.compiler }}" == "intel-oneapi-compilers" ]]; then
            . /opt/intel/oneapi/setvars.sh
          fi
          spack -e . compiler find && spack -e . compiler list

          spack -e . compiler info "${{ matrix.compiler }}"

      # Spack and oneAPI-2025.3.1 have some compatibility problems where the
      # version of the fortran compiler is not correctly identified. As a
      # workaround, we manually edit the relevant entry in the spack.yaml to put
      # all the compilers under the same version. # This amounts to merging two
      # entries (for two different versions) into a single one.
      #
      # TODO: Check if this issue is still relevant, if not, remove this step.
      - name: Merge Intel Compiler Entries
        if: needs.filter.outputs.test == 'true' && matrix.compiler == 'intel-oneapi-compilers'
        run: |
          python3 << 'EOF'
          import yaml
          
          with open('spack.yaml', 'r') as f:
              data = yaml.safe_load(f)
          
          # Get Intel compiler externals
          intel_externals = data['spack']['packages']['intel-oneapi-compilers']['externals']
          
          if len(intel_externals) > 1:
              # Merge compilers from all entries
              merged_compilers = {}
              newest_spec = None
              newest_prefix = None
              
              for entry in intel_externals:
                  # Keep track of the newest version
                  if newest_spec is None or entry['spec'] > newest_spec:
                      newest_spec = entry['spec']
                      newest_prefix = entry['prefix']
                  
                  # Merge all compilers
                  if 'extra_attributes' in entry and 'compilers' in entry['extra_attributes']:
                      compilers = entry['extra_attributes']['compilers']
                      merged_compilers.update(compilers)
              
              # Create single merged entry
              merged_entry = {
                  'spec': newest_spec,
                  'prefix': newest_prefix,
                  'extra_attributes': {
                      'compilers': merged_compilers
                  }
              }
              
              # Replace ALL entries with single merged entry
              data['spack']['packages']['intel-oneapi-compilers']['externals'] = [merged_entry]
              
              # Write back
              with open('spack.yaml', 'w') as f:
                  yaml.dump(data, f, default_flow_style=False)
              
              print("Merged Intel compiler entries into single entry")
          EOF

      - name: Display spack.yaml
        if: needs.filter.outputs.test == 'true'
        run: |
          echo "=== Generated spack.yaml ==="
          cat spack.yaml
          echo "=== End spack.yaml ==="

      - name: Configure Binary Mirror Keys
        if: needs.filter.outputs.test == 'true'
        run: |
          # If we cached these, that would be faster and safer
          spack -e . buildcache keys --install --trust

      - name: Bootstrap
        if: needs.filter.outputs.test == 'true'
        run: spack -e . bootstrap now

      - name: Concretize
        if: needs.filter.outputs.test == 'true'
        # In theory we can re-use a concretization and pin a spack to speed this up.
        # Unfortunately it then becomes difficult to know when to re-concretize.
        env:
          PALACE_REF: ${{ github.head_ref || github.ref_name }}
        run: |
          # Using `spack develop` in order to have an in-source build
          spack -e . develop --path=$(pwd) local.palace@git."${PALACE_REF}"=develop
          # --test root pulls in the test dependencies
          spack -e . concretize -f --test root
      - name: Build Dependencies
        if: needs.filter.outputs.test == 'true'
        run: |
          spack -e . install --only-concrete --no-check-signature --fail-fast --show-log-on-error --only dependencies --test root

      - name: Build Palace
        if: needs.filter.outputs.test == 'true'
        # Build palace from source using this current directory
        # We use `--no-cache` in order to force a rebuild
        # --test root installs the tests for the root specs (Palace)
        run: spack -e . install --only-concrete --keep-stage --show-log-on-error --only package --no-cache --test root

      - name: Run Unit Tests
        if: needs.filter.outputs.test == 'true'
        run: |
          eval $(spack -e . load --sh palace)

          if [[ "${{ matrix.cuda }}" == "+cuda" ]]; then
            export GPU_FLAGS="--device cuda"
          else
            export GPU_FLAGS=""
          fi

          if [[ "${{ matrix.openmp }}" == '+openmp' ]]; then
            export OMP_NUM_THREADS=2
          else
            export OMP_NUM_THREADS=1
          fi

          palace-unit-tests --skip-benchmarks ${GPU_FLAGS}
          mpirun -np 2 $(which palace-unit-tests) --skip-benchmarks ${GPU_FLAGS}

      - name: Run Integration Tests
        if: needs.filter.outputs.test == 'true'
        run: |
          # Use nproc, but halve to account for hyperthreading.
          LOGICAL_CORES=$(nproc)
          THREADS_PER_CORE=$(lscpu | grep "Thread(s) per core" | awk '{print $4}')
          if [ "$THREADS_PER_CORE" = "2" ]; then
              export NUM_PROC_TEST=$((LOGICAL_CORES / 2))
          else
              export NUM_PROC_TEST=$LOGICAL_CORES
          fi

          if [[ "${{ matrix.openmp }}" == '+openmp' ]]; then
            NUM_PROC_TEST=$(( NUM_PROC_TEST / 2 ))
            export OMP_NUM_THREADS=2
          else
            export OMP_NUM_THREADS=1
          fi

          eval $(spack -e . load --sh palace)
          # Run tests
          julia --project=test/examples -e 'using Pkg; Pkg.instantiate()'
          if [[ "${{ matrix.cuda }}" == +"cuda" ]]; then
            export TEST_GPU=1
            export NUM_PROC_TEST=1
          fi
          julia --project=test/examples --color=yes test/examples/runtests.jl

      # Push built binaries, even if the build fails
      # NOTE: This might fail as external fork PRs can't push to GHCR.
      - name: Push to GHCR cache
        if: |
          needs.filter.outputs.test == 'true' &&
          !cancelled() &&
          (github.event_name != 'pull_request' || github.event.pull_request.head.repo.full_name == github.repository)
        run: spack -e . buildcache push --force --with-build-dependencies --unsigned --update-index local-buildcache
